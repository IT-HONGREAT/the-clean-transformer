# --- config for training a model --- #
train:
  overfit_all:
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 100
    max_length: 145
    batch_size: 64
    lr: 0.0001
    tokenizer: "wp"
    dropout: 0.0
    seed: 410
    # shuffle the training set
    shuffle: true
    overfit_batches: 0
  # just to test things out
  overfit_10:
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 200
    max_length: 145
    batch_size: 64
    lr: 0.0001
    tokenizer: "wp"
    dropout: 0.0
    seed: 410
    # shuffle the training set
    shuffle: true
    overfit_batches: 10

# --- config for building a tokenizer --- #
build:
  vocab_size: 20000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

