# --- config for training a model --- #
train:
  first:
    hidden_size:  64
    heads: 8
    depth: 3
    max_epochs: 2
    max_length: 150
    batch_size: 64
    val_ratio: 0.15
    test_ratio: 0.15
    lr: 0.0001
    tokenizer: "wp"
    dropout: 0.0001
    seed: 410
    shuffle: true
  overfit:
    hidden_size: 32
    heads: 8
    depth: 3
    max_epochs: 50
    max_length: 150
    batch_size: 128
    val_ratio: 0.15
    test_ratio: 0.15
    lr: 0.00001
    tokenizer: "wp"
    dropout: 0.0
    seed: 410
    # shuffle the training set
    shuffle: true

# --- config for building a tokenizer --- #
build:
  vocab_size: 20000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

