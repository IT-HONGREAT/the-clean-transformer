# --- config for training a model --- #
train:
  overfit_torch:
    implementation: torch
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 10
    max_length: 145
    batch_size: 64
    tokenizer: wp
    dropout: 0.0
    seed: 410
    shuffle: true
    data: kor2eng
    lr: 1e-4
    beta_1: 0.9
    beta_2: 0.98
    eps: 1e-9
    warmup_steps: 4000
  # just to test things out
  overfit_small_torch:
    implementation: torch
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 400
    max_length: 145
    batch_size: 64
    tokenizer: wp
    dropout: 0.0
    seed: 410
    shuffle: true
    data: kor2eng_small
    lr: 1e-4
    beta_1: 0.9
    beta_2: 0.98
    eps: 1e-9
    warmup_steps: 4000
  overfit_scratch:
    implementation: scratch
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 10
    max_length: 145
    batch_size: 64
    tokenizer: wp
    dropout: 0.0
    seed: 410
    shuffle: true
    data: kor2eng
    lr: 1e-4
    beta_1: 0.9
    beta_2: 0.98
    eps: 1e-9
    warmup_steps: 4000
  # just to test things out
  overfit_small_scratch:
    implementation: scratch
    hidden_size: 512
    ffn_size: 512
    heads: 32
    depth: 3
    max_epochs: 400
    max_length: 145
    batch_size: 64
    tokenizer: wp
    dropout: 0.0
    seed: 410
    shuffle: true
    data: kor2eng_small
    lr: 1e-4
    beta_1: 0.9
    beta_2: 0.98
    eps: 1e-9
    warmup_steps: 4000

# --- config for building a tokenizer --- #
build:
  vocab_size: 20000
  pad: "[PAD]"
  pad_id: 0
  unk: "[UNK]"
  unk_id: 1
  bos: "[BOS]"
  bos_id: 2
  eos: "[EOS]"
  eos_id: 3

